# Processamento e Reconhecimento de Sinais de Libras 

## Sum√°rio
- [Sobre](#Sobre)
- [Motivativa√ß√£o](#Motiva√ß√£o)
- [Objetivos](#Objetivos)
- [Metologia](#Metologia)
    - [Achando uma Base de Dados](#Achando-uma-Base-de-Dados)
    - [Modelos Utilizados](#Modelos-Utilizados)
    - [Pr√©-Processamento](#Pr√©-Processamento)
    - [Treinamento dos Modelos](#Treinamento-dos-Modelos)
- [Resultados](#Resultados)
    - [Gr√°ficos](#Gr√°ficos)
    - [Predi√ß√µes](#Predi√ß√µes)
- [Conclus√£o](#Conclus√£o)
- [Agradecimentos](#Agradecimentos)
- [Autores](#Autores)
- [Licen√ßa](#Licen√ßa)
## üöÄ Sobre
O projeto se trata de uma ferramenta de reconhecimento e processamento de sinais da L√≠ngua Brasileira de Sinais (Libras) em um ambiente virtual de aprendizado para alunos que gostariam de aprender a reproduzi-los. Assim, o v√≠deo desenvolvido pelo aluno ser√° enviado para a plataforma e ser√° realizada uma valida√ß√£o se o sinal est√° de acordo com o banco de dados, dessa forma, entregando ao aluno se a reprodu√ß√£o est√° de acordo.


## üéóÔ∏è Motiva√ß√£o
A L√≠ngua Brasileira de Sinais √© reconhecida como um dos pilares da inclus√£o e da igualdade de direitos para pessoas surdas, garantindo-lhes acesso √† educa√ß√£o, informa√ß√£o, cultura e mercado de trabalho. No entanto, ainda existem desafios significativos relacionados √† sua difus√£o e compreens√£o por parte da popula√ß√£o em geral. Nesse contexto, a aplica√ß√£o de tecnologias de IA, como sistemas de reconhecimento e processamento de sinais de Libras, representa um avan√ßo revolucion√°rio para facilitar a aprendizagem e promover a acessibilidade comunicativa de forma eficiente e escal√°vel. 

J√° a motiva√ß√£o deste trabalho surge ap√≥s a participa√ß√£o do projeto de extens√£o GAAL (Games de apoio √† aprendizagem em Libras). Durante esse projeto, foi experienciado v√°rias dificuldades na cria√ß√£o de Serious Games - jogos que possuem fins educativos como foco principal, al√©m do entretenimento - em que atendam √†s recomenda√ß√µes dos professores de Libras e tenham um design atraente e divertido para os alunos ouvintes e n√£o-ouvintes, da escola parceira deste projeto.
## üìù Objetivos
* Estudar como funcionam as t√©cnicas de Deep Learning;
* Estudar diferentes modelos de rede neurais convolucionais;
* Analisar e discutir sobre a acur√°cia do modelo utilizado para reconhecer o acesso a materiais de educa√ß√£o inclusivos no Brasil.
## üî¨ Metodologia
### Passos
* **Achando uma Base de Dados**  
A base usada para este projeto foi a [V-Librasil](https://libras.cin.ufpe.br/). A base de dados possui por volta de 1350 sinais diferentes feita por 3 articuladores, totalizando em um pouco mais de 4000 v√≠deos para o treinamento. 

* **Modelos Utilizados**
Baseando-se na literatura vistas, os modelos escolhidos foram:

1. **Inception ResNetV2**
Combina a aequitetura Inception, que utiliza blocos convolucionais com diferentestamanhos de filtros para capturar v√°rias caracter√≠sticas da imagem, com a arquitetura ResNet, que emprega conex√µes residuais para facilitar o treinamento de redes profundas. √â eficiente para tarefas de classifica√ß√£o de imagens complexas devido √† sua capacidade de extrair caracter√≠sticas detalhadas 

2. **ResNet50V2** 
√â uma vers√£o aprimorada do modelo ResNet50, que utiliza conex√µes residuais para mitigar o problema de degrada√ß√£o em redes muito profundas. A vers√£o V2 ajusta a ordem das opera√ß√µes nos blocos residuais, o que melhora a converg√™ncia e o desempenho em tarefas de classifica√ß√£o de imagens.


3. **MobileNetV2**
Focado em efici√™ncia e desempenho em dispositivos m√≥veis, esse modelo utiliza blocos de convolu√ß√£o profundos e separ√°veis para reduzir o n√∫mero de par√¢metros sem sacrificar a precis√£o. √â ideal para aplica√ß√µes que requerem modelos leves e r√°pidos, como a execu√ß√£o em dispositivos com recursos limitados

* **Pr√©-processamento**
Atrav√©s do OpenCV, foi poss√≠vel o processamento dos sinais de
libras, j√° que essa biblioteca oferece uma s√©rie de ferramentas e
algoritmos para opera√ß√µes como detec√ß√£o de rostos,
reconhecimento de objetos, rastreamento de movimentos,
an√°lise de fluxo √≥ptico.

Foi feito rotula√ß√£o de todos os videos e criado um diret√≥rio para cada sinal distinto. Sinais que possuiam mesmo sinonimo ficaram juntos no mesmo diret√≥rio.

**Sele√ß√£o de quadros**: Foram selecionados, aleatoriamente, 50
quadros de cada v√≠deo usando uma distribui√ß√£o normal, que se
mostrou melhor que a uniforme nos experimentos.
Acreditamos que isso se deve ao fato de os v√≠deos j√° estarem
pr√©-processados e centralizados temporalmente.

O **conjunto de dados** selecionado foi o seguinte: 
* **Produtos de Mercado**: Tomate, Carne, P√£o, Manteiga e Caf√©.
* **Autom√≥veis**: Motocicleta, Helic√≥ptero, Bicicleta, Trem.
* **Pr√©dios**: Restaurante, Bombeiro, Hospital, Casa e Escola

Todos os videos foram redimensionados para 224x224 para o treinamento do modelo.

**Divis√£o dos conjuntos de treino, valida√ß√£o e teste**: A divis√£o
dos conjuntos de treino √© de 70%, a valida√ß√£o √© de 15% e o
teste √© de 15%. As observa√ß√µes de valida√ß√£o e teste foram
exclusivamente do conjunto de dados da UFPE, pois √© a √∫nica
fonte que possui pelo menos tr√™s observa√ß√µes por categoria.
Assim, todas as fontes estavam presentes no conjunto de
treino.


* **Treinamento dos Modelos**


**M√°ximo de √âpocas** =  50 \
**Dropout** = 0.5 \
**Batch Size** = 32 \
**Early Stopping** = 5

|       | **Inception ResNetV2** | **ResNet50V2**     | **MobileNetV2** |
| :---  |    :----:              |    :---:           |        :---:   |
|**Accuracy** | 0.0538 (5%)   | 0.2608 (27%)   |  0.9742 (97%)         |
|**Loss**   |   2.6392       | 2.0398      |       0.0898         |
|**Val Accuracy** | 0.0729 (7%)   | 0.6562 (65%)   |  0.9861 (98%)         |
|**Val Loss**   |   2.6393       | 1.7757      |       0.0654         |
|**√âpocas rodadas**|6|20|26|




## üìâ Resultados

A an√°lise dos resultados demonstra que o modelo **MobileNetV2** obteve o melhor desempenho, com uma acur√°cia de 97% e uma perda de 0,0898 ap√≥s 26 √©pocas, confirmando sua capacidade de generaliza√ß√£o e efici√™ncia no treinamento. O **Inception ResNetV2**, por outro lado, apresentou um desempenho significativamente inferior, com uma acur√°cia de apenas 5% e uma perda elevada de 2,6392, indicando que o modelo n√£o conseguiu aprender adequadamente, mesmo ap√≥s 6 √©pocas. O **ResNet50V2** alcan√ßou uma acur√°cia intermedi√°ria de 27%, mas a perda ainda foi alta, sugerindo que o modelo necessitaria de ajustes para melhorar seu aprendizado. Com base na compara√ß√£o dos tempos de processamento, optou-se por utilizar os tr√™s modelos de forma combinada. Esta escolha reflete uma estrat√©gia para equilibrar efici√™ncia e robustez no sistema.

### Gr√°ficos
* **ROC Curve**
**MobileNetV2**
![ROC Curve from MobileNetV2 Model](.\prints\Curva_roc_mobile.png)

**ResNet50V2**
![ROC Curve from ResNet50V2 Model](.\prints\Curva_roc_resNet.png)

**InceptionResNetV2**
![ROC Curve from InceptionResNetV2 Model](.\prints\Curva_roc_inception.png)

* **Precision and Recall**

**MobileNetV2**
![PR from MobileNetV2 Model](.\prints\Precision_and_recall_mobile.png)

**ResNet50V2**
![PR from ResNet50V2 Model](.\prints\Precision_and_recall_resNet.png)

**InceptionResNetV2**
![PR from InceptionResNetV2 Model](.\prints\Precision_and_recall_inception.png)



* **Train vs Validation Accuracy**

**MobileNetV2**
![Accuracy from MobileNetV2 Model](.\prints\train_val_accuracy_mobile.png)


**ResNet50V2**
![Accuracy from ResNet50V2 Model](.\prints\train_val_accuracy_resNet.png)

**InceptionResNetV2**
![Accuracy from InceptionResNetV2 Model](prints/Curva_roc_inception.png)

* **Train vs Validation Loss**

**MobileNetV2**
![Loss from MobileNetV2 Model](.\prints\train_val_loss_mobile.png)


**ResNet50V2**
![Loss from ResNet50V2 Model](.\prints\train_val_loss_resNet.png)

**InceptionResNetV2**
![Loss from InceptionResNetV2 Model](.\prints\train_val_loss_inception.png)


### Predi√ß√µes
- Good Examples
![Trem](.\prints\screenshot_7.png)

![Escola](.\prints\screenshot_1.png)

- Bad Examples
![Pao](.\prints\screenshot_5.png)

![Bombeiro](.\prints\screenshot_3.png)


## üåü Conclus√£o

O desenvolvimento desta ferramenta de reconhecimento e
processamento de sinais da L√≠ngua Brasileira de Sinais
(Libras), em um ambiente virtual de aprendizado,
representa um avan√ßo significativo no apoio √† educa√ß√£o
de Libras. A ferramenta de intelig√™ncia artificial criada
permite que os alunos enviem v√≠deos de suas pr√°ticas de
sinais, os quais s√£o validados em rela√ß√£o a um banco de
dados robusto, oferecendo feedback a respeito da
execu√ß√£o do usu√°rio.

At√© o presente momento, √© poss√≠vel observar que existem
altas expectativas para que esse modelo funcione e possua
uma boa acur√°cia. No relat√≥rio de formaliza√ß√£o, a ideia
inicial seria que o usu√°rio pudesse reproduzir o sinal ao
vivo, por√©m, ao serem analisadas as dificuldades por tr√°s
do processo, √© not√°vel que o ideal ser√° que o usu√°rio
envie um v√≠deo para que, esse sim, seja validado.
Enquanto isso, o modelo de reconhecimento de sinais ao
vivo ter√° que ser trabalhado futuramente.

Al√©m disso, com as novas avalia√ß√µes, notamos que os
landmarks n√£o ser√£o poss√≠veis, uma vez que o n√≠vel de
execu√ß√£o faria com que o projeto demorasse muito mais
para ser conclu√≠do. No geral, o abandono do uso de
landmarks e reestrutura√ß√£o do conjunto de dados usado
foram os passos ideais para que o projeto fosse poss√≠vel,
mas ser√£o mudan√ßas super vi√°veis para que projetos futuros
sejam realizados.

Portanto, a ferramenta tem demonstrado um progresso
promissor na valida√ß√£o autom√°tica de sinais de Libras,
aproximando-se de oferecer resultados √∫teis e aplic√°veis no
contexto educacional.

As melhorias cont√≠nuas e a amplia√ß√£o do banco de dados
ser√£o cruciais para aprimorar a efic√°cia e a precis√£o da
plataforma, consolidando-a como uma ferramenta valiosa
para estudantes de Libras. Este projeto contribui
significativamente para a inclus√£o digital e a acessibilidade
educacional, promovendo o aprendizado aut√¥nomo e eficaz
da L√≠ngua Brasileira de Sinais.

## Agradecimentos

Gostar√≠amos de expressar nossa sincera gratid√£o ao Banco de
Dados do [V-Librasil](https://libras.cin.ufpe.br/), que forneceu recursos essenciais para o
desenvolvimento da pesquisa cient√≠fica de Libras no Brasil.
Agradecemos tamb√©m ao Professor F√°bio Faria, cuja
orienta√ß√£o e apoio foram fundamentais ao longo deste projeto.
Nosso reconhecimento especial ao monitor Christian, por sua
dedica√ß√£o e assist√™ncia cont√≠nua, que contribu√≠ram
significativamente para o sucesso deste trabalho.

## Autores

- [@gssabbado](https://github.com/gssabbado)
- [@gabituzaki](https://github.com/gabituzaki)


## Licen√ßa

[MIT](https://choosealicense.com/licenses/mit/)

